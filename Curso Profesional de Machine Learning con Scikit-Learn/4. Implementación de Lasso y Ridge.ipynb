{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             rank       score        high         low         gdp      family  \\\n",
      "count  155.000000  155.000000  155.000000  155.000000  155.000000  155.000000   \n",
      "mean    78.000000    5.354019    5.452326    5.255713    0.984718    1.188898   \n",
      "std     44.888751    1.131230    1.118542    1.145030    0.420793    0.287263   \n",
      "min      1.000000    2.693000    2.864884    2.521116    0.000000    0.000000   \n",
      "25%     39.500000    4.505500    4.608172    4.374955    0.663371    1.042635   \n",
      "50%     78.000000    5.279000    5.370032    5.193152    1.064578    1.253918   \n",
      "75%    116.500000    6.101500    6.194600    6.006527    1.318027    1.414316   \n",
      "max    155.000000    7.537000    7.622030    7.479556    1.870766    1.610574   \n",
      "\n",
      "           lifexp     freedom  generosity  corruption    dystopia  \n",
      "count  155.000000  155.000000  155.000000  155.000000  155.000000  \n",
      "mean     0.551341    0.408786    0.246883    0.123120    1.850238  \n",
      "std      0.237073    0.149997    0.134780    0.101661    0.500028  \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.377914  \n",
      "25%      0.369866    0.303677    0.154106    0.057271    1.591291  \n",
      "50%      0.606042    0.437454    0.231538    0.089848    1.832910  \n",
      "75%      0.723008    0.516561    0.323762    0.153296    2.144654  \n",
      "max      0.949492    0.658249    0.838075    0.464308    3.117485  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/felicidad.csv')\n",
    "print(df.describe()) #imprimimos una descripción estadística de cada una de nuestras columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 7)\n",
      "(155,)\n"
     ]
    }
   ],
   "source": [
    "df_features =df[['gdp', 'family', 'lifexp', 'freedom', 'corruption', 'generosity', 'dystopia']]\n",
    "df_target = df['score']\n",
    "print(df_features.shape)\n",
    "print(df_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df_features, df_target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLinear = LinearRegression().fit(X_train, Y_train)\n",
    "y_predict_linear = modelLinear.predict(X_test)\n",
    "\n",
    "modelLasso =Lasso(alpha=0.022).fit(X_train, Y_train)\n",
    "y_predict_lasso = modelLasso.predict(X_test)\n",
    "#Recuerda que alpha es nuestro \"lambda\" de nuestro regulador Lasso(). Por defecto está en 1 \n",
    "#Mientras más grande sea alpha, mayor va a ser la penalización para nuestros features.\n",
    "\n",
    "modelRidge = Ridge(alpha=1).fit(X_train, Y_train)   \n",
    "y_predict_Ridge = modelRidge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo vamos a calcular la pérdida con **El Error Medio Cuadrático ( MSE )**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Loss: 1.0003997491914725e-07\n",
      "Lasso Loss: 0.044447130657402655\n",
      "Ridge Loss: 0.004661867276719037\n"
     ]
    }
   ],
   "source": [
    "linear_loss = mean_squared_error(Y_test, y_predict_linear)\n",
    "print('Linear Loss:', linear_loss)\n",
    "\n",
    "lasso_loss = mean_squared_error(Y_test, y_predict_lasso)\n",
    "print('Lasso Loss:', lasso_loss)\n",
    "\n",
    "ridge_loss = mean_squared_error(Y_test, y_predict_Ridge)\n",
    "print('Ridge Loss:', ridge_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obervar que el modelo de regresión lineal tiene la menor pérdida entre los otros modelos.\n",
    "Por otro lado, si solo comparamos los dos modelos regulados, observamos que el modelo Ridge nos retorna una perdida menor para este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef Lineal\n",
      "[1.00016148 0.99995068 0.99964178 0.99981154 0.99995186 1.00026985\n",
      " 0.999953  ]\n",
      "Coef Lasso\n",
      "[1.30792215 0.78367152 0.48287212 0.77991024 0.         0.17904496\n",
      " 0.90404044]\n",
      "Coef Ridge\n",
      "[1.07420543 0.93594342 0.86652574 0.89967036 0.6702908  0.76622235\n",
      " 0.96444973]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coef Lineal\")\n",
    "print(modelLinear.coef_)\n",
    "print(\"Coef Lasso\")\n",
    "print(modelLasso.coef_)\n",
    "print(\"Coef Ridge\")\n",
    "print(modelRidge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los resultados podemos comprobar la hipótesis que sostenía que el modelo Lasso tiende a reducir los pesos de nuestros features a 0 y que el modelo Ridge tiende solo a reducirlos.\n",
    "\n",
    "Por otro lado, nuestro modelo lineal no hace ello ( es ovio porque no aplica regularización alguna a los features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Lineal\n",
      "0.9999999231608282\n",
      "Score Lasso\n",
      "0.9658608399998099\n",
      "Score Ridge\n",
      "0.9964192911779547\n"
     ]
    }
   ],
   "source": [
    "print(\"Score Lineal\")\n",
    "print(modelLinear.score(X_test, Y_test))\n",
    "print(\"Score Lasso\")\n",
    "print(modelLasso.score(X_test, Y_test))\n",
    "print(\"Score Ridge\")\n",
    "print(modelRidge.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, observamos que el mejor Score lo obtenemos de nuestro modelo Lineal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
